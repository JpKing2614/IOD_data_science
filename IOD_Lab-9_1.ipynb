{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gn3SG1yo1KdV"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.5.0-cp38-cp38-macosx_10_11_x86_64.whl (195.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 195.7 MB 45 kB/s  eta 0:00:01     |█████████████████               | 103.5 MB 977 kB/s eta 0:01:35     |████████████████████████▍       | 149.4 MB 994 kB/s eta 0:00:47███████████████       | 153.0 MB 632 kB/s eta 0:01:08�█████████████████████      | 158.5 MB 946 kB/s eta 0:00:40\n",
      "\u001b[?25hRequirement already satisfied: six~=1.15.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.17.3-cp38-cp38-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy~=1.19.2 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.19.2)\n",
      "Collecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp38-cp38-macosx_10_10_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting tensorboard~=2.5\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp38-cp38-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (2.24.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.5-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 16.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.34.0-py2.py3-none-any.whl (152 kB)\n",
      "\u001b[K     |████████████████████████████████| 152 kB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (57.4.0)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 5.2 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.0)\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=4bb7beb30960b95909514883015ed7dde5e035e39e52f617a29e73d4aca357df\n",
      "  Stored in directory: /Users/jamesking/Library/Caches/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-macosx_10_9_x86_64.whl size=32765 sha256=b4976194ffdc3d5a2f9a554d89dbfc0ac42e463ca33c3eace9506a3e016da2b6\n",
      "  Stored in directory: /Users/jamesking/Library/Caches/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, cachetools, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras-nightly, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.11.2\n",
      "    Uninstalling wrapt-1.11.2:\n",
      "      Successfully uninstalled wrapt-1.11.2\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Uninstalling h5py-2.10.0:\n",
      "      Successfully uninstalled h5py-2.10.0\n",
      "Successfully installed absl-py-0.13.0 astunparse-1.6.3 cachetools-4.2.2 flatbuffers-1.12 gast-0.4.0 google-auth-1.34.0 google-auth-oauthlib-0.4.5 google-pasta-0.2.0 grpcio-1.34.1 h5py-3.1.0 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 markdown-3.3.4 opt-einsum-3.3.0 protobuf-3.17.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 rsa-4.7.2 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.5.0 tensorflow-estimator-2.5.0 termcolor-1.1.0 wrapt-1.12.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-SD7a9X1KdY"
   },
   "source": [
    "# Lab 9.1: NN with Keras\n",
    "INSTRUCTIONS:\n",
    "- Read the guides and hints, then create the necessary analysis and code to find an answer and conclusion for the task below.\n",
    "- **NOTE**: This is a Regression problem. Consider the appropriate:\n",
    "    - Activation function\n",
    "    - Loss/Cost Function\n",
    "    - Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ENgfRnvL1Kdc"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9H465X-1Kde"
   },
   "outputs": [],
   "source": [
    "# insert code here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "#from keras.utils import to_categorical\n",
    "import keras.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5FHh910J1Kdm"
   },
   "source": [
    "### Load data\n",
    "Load the Diabetes dataset from **SciKit-Learn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWu8SlQF1Kdo"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  \n",
       "0 -0.002592  0.019908 -0.017646  \n",
       "1 -0.039493 -0.068330 -0.092204  \n",
       "2 -0.002592  0.002864 -0.025930  \n",
       "3  0.034309  0.022692 -0.009362  \n",
       "4 -0.002592 -0.031991 -0.046641  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert code here\n",
    "diabetes_data = datasets.load_diabetes()\n",
    "diabetes_df = pd.DataFrame(diabetes_data.data, columns=diabetes_data.feature_names)\n",
    "\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xR_yLpR01Kdr"
   },
   "source": [
    "### Prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sp2mf2bB1Kds"
   },
   "outputs": [],
   "source": [
    "# insert code here\n",
    "X = diabetes_df.values\n",
    "\n",
    "y = diabetes_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "        69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
       "        68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
       "        87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
       "       259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
       "       128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
       "       150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
       "       200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
       "        42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
       "        83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
       "       104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
       "       173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
       "       107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
       "        60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
       "       197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
       "        59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
       "       237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
       "       143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
       "       142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
       "        77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
       "        78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
       "       154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
       "        71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
       "       150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
       "       145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
       "        94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
       "        60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
       "        31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
       "       114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
       "       191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
       "       244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
       "       263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
       "        77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
       "        58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
       "       140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
       "       219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
       "        43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
       "       140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
       "        84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
       "        94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
       "       220.,  57.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1Vsh6cm1Kdv"
   },
   "source": [
    "### Split the data (training/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O341llJz1Kdw"
   },
   "outputs": [],
   "source": [
    "# insert code here\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o4dJViJD1Kd0"
   },
   "source": [
    "### Create the model's architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tkgf_BLl1Kd1"
   },
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid Function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of Sigmoid Function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# ReLU Function\n",
    "def relu(x):\n",
    "    return np.max(0, x)\n",
    "\n",
    "# Derivative of ReLU Function\n",
    "def relu_derivative(x):\n",
    "    return 0 if x < 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = X.shape[1]\n",
    "# Set up the model architecture\n",
    "model = Sequential()\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(100, activation = 'relu', input_shape = (n_cols, )))\n",
    "# Add the second hidden layer\n",
    "model.add(Dense(50, activation = 'relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5hQTS42V1Kd4"
   },
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BvLsoanF1Kd5"
   },
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = 'sgd',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6KPQbnEj1Kd7"
   },
   "source": [
    "### Fit the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-oiuHjEj1Kd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14/14 [==============================] - 5s 42ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 24/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 25/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "14/14 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 31/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 34/100\n",
      "14/14 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 37/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 42/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 44/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 46/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 49/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 52/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 53/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 54/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 56/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 57/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 59/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 64/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 67/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 68/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 69/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 79/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 80/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 81/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 83/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 84/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 85/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 86/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 89/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 90/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 91/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 92/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 93/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 94/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 95/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 96/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 97/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 98/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 99/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 100/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "CPU times: user 9.41 s, sys: 665 ms, total: 10.1 s\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# insert code here\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split = 0.25,\n",
    "    batch_size = 20,\n",
    "    epochs = 100,\n",
    "    verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CR-kCu3c1Kd-"
   },
   "source": [
    "### Create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkywKqPg1Kd-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert code here\n",
    "predictions = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-unEmrGo1KeA"
   },
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QCIVpVrG1KeB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "\n",
      "Test loss: nan, Test accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# insert code here\n",
    "score = model.evaluate(X_test, y_test, batch_size = 315)\n",
    "print('\\nTest loss: %.6f, Test accuracy: %.6f' % tuple(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSlqjs7e1KeD"
   },
   "source": [
    "### Visualisation of cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1F6hYpNh1KeE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cm(cm):\n",
    "    c = '%%%dd ' % len('%d' % cm.max())\n",
    "    s = ' | '\n",
    "    s += ''.join([c % i for i in range(len(cm[0]))])\n",
    "    print(s)\n",
    "    print('-' * len(s))\n",
    "    for i, r in enumerate(cm):\n",
    "        s = '%d| ' % i\n",
    "        s += c * len(r)\n",
    "        print(s % tuple(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-d6f7ada65eb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Plot training & validation accuracy values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (18, 6))\n",
    "fig.subplots_adjust(left = 0.02, right = 0.98, wspace = 0.2)\n",
    "\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "ax[0].plot(model.history.history['accuracy'], label = 'Training')\n",
    "ax[0].plot(model.history.history['val_accuracy'], label = 'Validation')\n",
    "ax[0].set_title('Model accuracy')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "ax[1].plot(model.history.history['loss'], label = 'Training')\n",
    "ax[1].plot(model.history.history['val_loss'], label = 'Validation')\n",
    "ax[1].set_title('Model loss')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vQLzMZTM1KeG"
   },
   "source": [
    "### Visualisation of residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opIulVSw1KeH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 67., 275., 175., ..., 101.,  95., 208.],\n",
       "       [ 67., 275., 175., ..., 101.,  95., 208.],\n",
       "       [ 67., 275., 175., ..., 101.,  95., 208.],\n",
       "       ...,\n",
       "       [ 67., 275., 175., ..., 101.,  95., 208.],\n",
       "       [ 67., 275., 175., ..., 101.,  95., 208.],\n",
       "       [ 67., 275., 175., ..., 101.,  95., 208.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert code here\n",
    "residuals = y_test-predictions\n",
    "residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RERADKgNFq9T"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > © 2021 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DSIA Lab-10_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
